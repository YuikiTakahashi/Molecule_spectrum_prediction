# GPU Acceleration Architecture - Technical Details

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Notebook Execution Flow                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Cell 4: GPU Configuration & Patching      â”‚
        â”‚  - Detect CUDA availability (subprocess)  â”‚
        â”‚  - Patch EL.diagonalize() functions       â”‚
        â”‚  - Set up GPU_PROFILING tracking          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Cell 10: GPU Benchmark Tests              â”‚
        â”‚  - Test single matrix eigendecomposition  â”‚
        â”‚  - Test batch operations on GPU           â”‚
        â”‚  - Report CPU vs CUDA performance         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Cell 11: Realistic Workflow Benchmark     â”‚
        â”‚  - Small, Medium, Large matrix tests      â”‚
        â”‚  - Batch processing tests                 â”‚
        â”‚  - Expected speedup measurements          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Cell 17: X010_173 Initialization          â”‚
        â”‚  [GPU ACCELERATION ACTIVE] â† diagonalize()â”‚
        â”‚  Sets baseline state parameters           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Cells 22-24: Synthetic Peak Generation    â”‚
        â”‚  [GPU ACCELERATION ACTIVE] â† get_evals()  â”‚
        â”‚  For each spectrum: eigensystem()         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PROFILING CHECKPOINT 1                    â”‚
        â”‚  print_gpu_profile_checkpoint()           â”‚
        â”‚  Shows GPU usage so far                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Cell 30: Parameter Search & Fitting       â”‚
        â”‚  [GPU ACCELERATION ACTIVE] âš¡âš¡âš¡          â”‚
        â”‚  search_candidates_MAP() calls:           â”‚
        â”‚   - Sample 40+ parameter candidates       â”‚
        â”‚   - For each: total_loss_MAP()            â”‚
        â”‚   - For each: unassigned_multispectrum()  â”‚
        â”‚   - For each spectrum: transition_freq()  â”‚
        â”‚   - For each transition: eigensystem()    â”‚
        â”‚                                           â”‚
        â”‚  Total diagonalization calls:             â”‚
        â”‚   â‰ˆ 40 candidates Ã— 4 spectra Ã— 2 refine â”‚
        â”‚   = 320+ eigensystem() calls              â”‚
        â”‚   THIS IS WHERE GPU SHINES! ğŸ’ª           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PROFILING CHECKPOINT 2                    â”‚
        â”‚  print_gpu_profile_checkpoint()           â”‚
        â”‚  Final GPU usage statistics               â”‚
        â”‚  Shows total speedup achieved             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Cells 31+: Plotting & Analysis            â”‚
        â”‚  [GPU ACCELERATION ACTIVE] â† plot_cand()  â”‚
        â”‚  plot_candidate() calls compute_model_t() â”‚
        â”‚  which calls eigensystem()                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Diagonalization Call Flow

```
User Code (Spectral Fitting)
        â†“
search_candidates_MAP()
        â”œâ”€â†’ total_loss_MAP()
        â”‚    â”œâ”€â†’ apply_params_partial()
        â”‚    â”‚    â””â”€â†’ set_state_parameters()
        â”‚    â”‚         â””â”€â†’ state.eigensystem() â† GPU CALL #1
        â”‚    â””â”€â†’ unassigned_multispectrum_loss()
        â”‚         â””â”€â†’ transition_frequency_set()
        â”‚              â””â”€â†’ compute_model_transitions()
        â”‚                  â””â”€â†’ state.calculate_two_photon_spectrum()
        â”‚                      â””â”€â†’ state.eigensystem() â† GPU CALL #2
        â””â”€â†’ [repeat for each candidate]

Each candidate evaluation = 2 eigensystem() calls
Each call â†’ diagonalize_with_device() [GPU if available]
```

## GPU Memory Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: Hermitian Matrix (float64)       â”‚
â”‚ Size: NÃ—N (e.g., 300Ã—300)               â”‚
â”‚ RAM: 720 KB per matrix                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Convert to  â”‚
         â”‚ float32 for â”‚
         â”‚ GPU (360KB) â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Transfer to GPU Memory      â”‚ â† Pinned memory transfer
    â”‚ (PCIe Gen3/4 ~10-50 GB/s)   â”‚
    â”‚ Time: ~1-10 Î¼s              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ torch.linalg.eigh()â”‚
         â”‚ On GPU             â”‚
         â”‚ Time: 1-100 ms     â”‚
         â”‚ (main computation) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Transfer back to CPU        â”‚ â† Pinned memory transfer
    â”‚ RAM as float64              â”‚
    â”‚ Time: ~1-10 Î¼s              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output: Eigenvalues + Eigenvectors   â”‚
â”‚ (float64, RAM)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Profiling Statistics Tracking

```python
GPU_PROFILING = {
    # Single matrix diagonalization
    "diagonalize_calls": int,        # Total calls to diagonalize()
    "diagonalize_cuda_calls": int,   # Calls on GPU
    "diagonalize_cpu_calls": int,    # Calls on CPU
    "total_time_cuda": float,        # Sum of GPU times (seconds)
    "total_time_cpu": float,         # Sum of CPU times (seconds)
    
    # Batch matrix diagonalization
    "diagonalize_batch_calls": int,        # Total batch calls
    "diagonalize_batch_cuda_calls": int,   # Batch calls on GPU
    "diagonalize_batch_cpu_calls": int,    # Batch calls on CPU
    "total_time_batch_cuda": float,        # Sum of GPU batch times
    "total_time_batch_cpu": float,         # Sum of CPU batch times
}

# Calculated metrics
GPU_fraction = (total_time_cuda + total_time_batch_cuda) / total_compute_time
Speedup = avg_cpu_time / avg_gpu_time  # Per-call speedup
```

## Performance Modeling

### Single Matrix Eigendecomposition
```
Time â‰ˆ Data_Transfer + Computation + Synchronization

Transfer Time â‰ˆ (2 Ã— Matrix_SizeÂ²) / PCIe_Bandwidth
             â‰ˆ (2 Ã— NÂ² Ã— 8 bytes) / (16 GB/s)
             â‰ˆ NÂ² Ã— 1 ns

Computation Time â‰ˆ O(NÂ³) (cubic complexity)
             â‰ˆ 2-10 ms for N=300
             â‰ˆ 20-100 ms for N=500

Synchronization â‰ˆ 1-10 Î¼s (minimal overhead)

GPU Sweet Spot: N â‰¥ 200 (computation dominates transfer)
```

### Batch Processing
```
Batch_Time â‰ˆ M Ã— Single_Matrix_Time + Small_Overhead

Where M = batch size (number of matrices)

GPU Utilization â†‘ with larger M
(Amortize launch overhead across more matrices)

Ideal: M â‰¥ 10 for efficient GPU occupancy
```

## CUDA Execution Timeline Example

For a 300Ã—300 matrix on NVIDIA RTX 4090:
```
Timeline:
  0.0 Î¼s: Python torch.from_numpy()
  5.0 Î¼s: .to('cuda') - Transfer to GPU
 10.0 Î¼s: torch.cuda.synchronize()
 10.5 Î¼s: torch.linalg.eigh() - Launch kernel
  â†“
 10.5 Î¼s + T_kernel: Kernel execution
          T_kernel â‰ˆ 2-5 ms (depending on matrix)
  â†“
 15.5 Î¼s: torch.cuda.synchronize()
 20.0 Î¼s: .cpu() - Transfer back
 20.0 Î¼s + T_transfer: CPU memory ready
          T_transfer â‰ˆ 1-10 Î¼s
  â†“
 20.5 Î¼s: Return to Python

Total wall-clock time â‰ˆ 2-5 ms (GPU computation dominates)
vs. 20-50 ms on CPU â†’ 5-10x speedup
```

## When GPU Provides Maximum Benefit

âœ… **Large parameter searches** (100+ candidates)
   â†’ 300+ eigensystem() calls
   â†’ 2-5 minute workflow becomes 10-30 seconds

âœ… **Large molecular systems** (N > 200)
   â†’ Heavier matrices
   â†’ Computation time dominates transfer overhead

âœ… **Multiple spectra per candidate**
   â†’ 4+ spectra Ã— 40 candidates = 160+ calls

âŒ **Small matrices** (N < 100)
   â†’ Transfer overhead comparable to computation
   â†’ GPU ~2-3x faster (not dramatic)

âŒ **Single evaluations** (1-2 eigensystem calls)
   â†’ Launch/transfer overhead significant
   â†’ May not see speedup (CPU fallback OK)

## Fallback Mechanism

```python
try:
    # Attempt GPU execution
    tensor = torch.from_numpy(arr).to(TORCH_DEVICE)
    w, v = torch.linalg.eigh(tensor)
    # Success â†’ use GPU result
except Exception as e:
    # GPU failed (out of memory, driver issue, etc.)
    print(f"Warning: GPU failed: {e}")
    # Seamlessly fallback to CPU
    return EL.diagonalize_cpu(matrix, method='numpy', ...)
    # Workflow continues without interruption âœ“
```

**Key Feature**: GPU failures do NOT stop your workflow.
Automatic fallback ensures robustness.

## Optimization Recommendations

1. **For fastest parameter search**:
   ```python
   # Increase batch size and candidates
   search_candidates_MAP(..., n_samples=200, top_k=20)
   # â†’ More GPU work = better amortization
   ```

2. **For monitoring GPU usage**:
   ```python
   # Run in separate terminal
   nvidia-smi -l 1  # Update every 1 second
   # Watch memory & utilization during fitting
   ```

3. **For profiling specific cells**:
   ```python
   print_gpu_profile_checkpoint("After Cell X")
   # Inserted after major sections
   ```

4. **For maximum compatibility**:
   ```python
   # Force CPU if GPU issues
   EL.TORCH_DEVICE = torch.device("cpu")
   # Or let automatic probe handle it âœ“
   ```

---

## Reference: Device Detection Code (Cell 4)

The system probes CUDA safety using a subprocess:

```python
probe_code = """
import torch
try:
    # Simple tensor creation & synchronization
    x = torch.tensor([1.0]).to('cuda')
    torch.cuda.synchronize()
    print('cuda_ok')
except Exception:
    print('cuda_fail')
    sys.exit(1)
"""

# This subprocess approach prevents:
# - Kernel panics if GPU is in bad state
# - Driver crashes during main notebook session
# - Hangs if GPU is unresponsive
```

If probe fails â†’ CUDA disabled, but notebook continues on CPU safely.

---

## Summary

**GPU Acceleration is:**
- âœ… Automatic (detected at start)
- âœ… Safe (fallback to CPU on any error)
- âœ… Transparent (no code changes needed)
- âœ… Profiled (real-time statistics)
- âœ… Most effective for large parameter searches (5-20x speedup)
