{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b4a450a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 2.9.1+cpu is available\n",
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sympy as sy\n",
    "\n",
    "# Try importing torch, but don't fail if it's not available\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(f\"PyTorch {torch.__version__} is available\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"Warning: PyTorch is not installed. Using numpy/scipy for diagonalization.\")\n",
    "    torch = None\n",
    "\n",
    "import ujson as uj\n",
    "\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "from sympy.physics.wigner import wigner_3j, wigner_6j\n",
    "from numpy import linalg as LA\n",
    "from IPython.display import Latex, display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.collections import LineCollection\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.stats import norm\n",
    "\n",
    "import Energy_Levels as EL\n",
    "from Energy_Levels import MoleculeLevels\n",
    "from Energy_Levels import (\n",
    "    branching_ratios,\n",
    "    Calculate_TDMs,\n",
    "    Calculate_TDM_evecs,\n",
    "    Calculate_forbidden_TDM_evecs,\n",
    "    Calculate_forbidden_TDMs,\n",
    ")\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "\n",
    "# sns.set()\n",
    "# sns.set_palette('bright')\n",
    "# np.set_printoptions(precision=9, suppress=True)\n",
    "# from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cadc8a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch device: cpu\n",
      "Diagonalization functions configured successfully.\n",
      "Default method: torch\n"
     ]
    }
   ],
   "source": [
    "# Torch device configuration and GPU-ready diagonalization hooks\n",
    "# Only configure torch if it's available\n",
    "if TORCH_AVAILABLE:\n",
    "    try:\n",
    "        TORCH_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using torch device: {TORCH_DEVICE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not configure torch device: {e}\")\n",
    "        TORCH_AVAILABLE = False\n",
    "else:\n",
    "    TORCH_DEVICE = None\n",
    "    print(\"Torch not available, using numpy/scipy for diagonalization\")\n",
    "\n",
    "# Store original functions\n",
    "if not hasattr(EL, \"diagonalize_cpu\"):\n",
    "    EL.diagonalize_cpu = EL.diagonalize\n",
    "    EL.diagonalize_batch_cpu = EL.diagonalize_batch\n",
    "\n",
    "# Patch the diagonalize function in the Energy_Levels module\n",
    "# This ensures internal calls to diagonalize() use our patched version\n",
    "def diagonalize_with_device(matrix, method=\"torch\", order=False, Normalize=False, round=10):\n",
    "    \"\"\"GPU/CPU-aware diagonalization with proper tensor handling.\"\"\"\n",
    "    if method == \"torch\" and TORCH_AVAILABLE:\n",
    "        try:\n",
    "            tensor = torch.from_numpy(matrix).to(TORCH_DEVICE)\n",
    "            w, v = torch.linalg.eigh(tensor)\n",
    "            # Use detach() to ensure we can convert to numpy even if requires_grad=True\n",
    "            evals = np.round(w.detach().cpu().numpy(), round)\n",
    "            evecs = np.round(v.detach().cpu().numpy().T, round)\n",
    "            if order:\n",
    "                idx_order = np.argsort(evals)\n",
    "                evecs = evecs[idx_order, :]\n",
    "                evals = evals[idx_order]\n",
    "            return evals, evecs\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Torch diagonalization failed: {e}\")\n",
    "            print(\"Falling back to numpy...\")\n",
    "            method = \"numpy\"\n",
    "    # Fallback to original function for non-torch methods\n",
    "    return EL.diagonalize_cpu(matrix, method=method, order=order, Normalize=Normalize, round=round)\n",
    "\n",
    "\n",
    "def diagonalize_batch_with_device(matrix_array, method=\"torch\", round=10):\n",
    "    \"\"\"GPU/CPU-aware batch diagonalization with proper tensor handling.\"\"\"\n",
    "    if method == \"torch\" and TORCH_AVAILABLE:\n",
    "        try:\n",
    "            tensors = torch.from_numpy(matrix_array).to(TORCH_DEVICE)\n",
    "            w, v = torch.linalg.eigh(tensors)\n",
    "            evals = np.round(w.detach().cpu().numpy(), round)\n",
    "            evecs = np.round(v.detach().cpu().numpy().transpose(0, 2, 1), round)\n",
    "            return evals, evecs\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Torch batch diagonalization failed: {e}\")\n",
    "            print(\"Falling back to numpy...\")\n",
    "            method = \"numpy\"\n",
    "    # Fallback to original function for non-torch methods\n",
    "    return EL.diagonalize_batch_cpu(matrix_array, method=method, round=round)\n",
    "\n",
    "\n",
    "# Patch the functions in the module namespace\n",
    "# This replaces the function references so internal calls use the patched version\n",
    "EL.diagonalize = diagonalize_with_device\n",
    "EL.diagonalize_batch = diagonalize_batch_with_device\n",
    "if TORCH_AVAILABLE:\n",
    "    EL.TORCH_DEVICE = TORCH_DEVICE\n",
    "\n",
    "print(\"Diagonalization functions configured successfully.\")\n",
    "print(f\"Default method: {'torch' if TORCH_AVAILABLE else 'numpy'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d3288a9-0e57-44b8-9b62-a9427c4ace56",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X010_173 \u001b[38;5;241m=\u001b[39m MoleculeLevels\u001b[38;5;241m.\u001b[39minitialize_state(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYbOH\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m173\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX010\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m      6\u001b[0m     M_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     I\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m      8\u001b[0m     S\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mround\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     10\u001b[0m     P_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kyoto\\Yuiki_github_test\\Molecule_spectrum_prediction\\Energy_Levels.py:97\u001b[0m, in \u001b[0;36mMoleculeLevels.initialize_state\u001b[1;34m(cls, molecule, isotope, state, N_range, M_values, I, S, P_values, M_range, round, trap, theta_num)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# Properties contain information relevant to the isotope and state of interest\u001b[39;00m\n\u001b[0;32m     82\u001b[0m properties \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmolecule\u001b[39m\u001b[38;5;124m'\u001b[39m: molecule,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso_state\u001b[39m\u001b[38;5;124m'\u001b[39m: iso_state,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta_num\u001b[39m\u001b[38;5;124m'\u001b[39m:theta_num\n\u001b[0;32m     96\u001b[0m }\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mproperties)\n",
      "File \u001b[1;32mc:\\Users\\kyoto\\Yuiki_github_test\\Molecule_spectrum_prediction\\Energy_Levels.py:126\u001b[0m, in \u001b[0;36mMoleculeLevels.__init__\u001b[1;34m(self, **properties)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_function, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_symbolic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mH_builders[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miso_state](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_numbers,M_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM_values,precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mround,theta_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta_num)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_function, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_symbolic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mH_builders[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miso_state](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_numbers,M_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM_values,precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mround)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mI_trap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta_num \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kyoto\\Yuiki_github_test\\Molecule_spectrum_prediction\\hamiltonian_builders.py:232\u001b[0m, in \u001b[0;36mH_odd_X\u001b[1;34m(q_numbers, params, matrix_elements, symbolic, E, B, M_values, precision)\u001b[0m\n\u001b[0;32m    230\u001b[0m state_in \u001b[38;5;241m=\u001b[39m {q\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m:q_numbers[q][j] \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m q_str}\n\u001b[0;32m    231\u001b[0m q_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstate_out,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstate_in}\n\u001b[1;32m--> 232\u001b[0m elements \u001b[38;5;241m=\u001b[39m {term: element(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mq_args) \u001b[38;5;28;01mfor\u001b[39;00m term, element \u001b[38;5;129;01min\u001b[39;00m matrix_elements\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    233\u001b[0m H0[i][j] \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBe\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39melements[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN^2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGamma_SR\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39melements[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN.S\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m    234\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbFYb\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39melements[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIM.S\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m6\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcYb\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39melements[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT2_0(IM,S)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m    235\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbFH\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39melements[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIH.S\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m6\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcH\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39melements[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT2_0(IH,S)\u001b[39m\u001b[38;5;124m'\u001b[39m]            \n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m#old, modified by Yuiki on 11/16/2022 because I changed the factor for 'T2_0(IH,S)' so we get the same '+ np.sqrt(6)/3' factor in front of 'cH' parameter as 'cYb'.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03mH0[i][j] = params['Be']*elements['N^2'] + params['Gamma_SR']*elements['N.S'] + \\\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m    params['bFYb']*elements['IM.S'] + np.sqrt(6)/3*params['cYb']*elements['T2_0(IM,S)'] +\\\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m    params['bFH']*elements['IH.S'] + (-np.sqrt(10))*params['cH']/3*elements['T2_0(IH,S)']\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kyoto\\Yuiki_github_test\\Molecule_spectrum_prediction\\matrix_elements.py:385\u001b[0m, in \u001b[0;36mStarkZ_bBS\u001b[1;34m(K0, N0, G0, F10, F0, M0, K1, N1, G1, F11, F1, M1, S, I, iH)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(F0\u001b[38;5;241m-\u001b[39mM0\u001b[38;5;241m+\u001b[39mF1\u001b[38;5;241m+\u001b[39mF10\u001b[38;5;241m+\u001b[39miH\u001b[38;5;241m+\u001b[39mF11\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mN0\u001b[38;5;241m+\u001b[39mG0\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m-\u001b[39mK0)\u001b[38;5;241m*\u001b[39mwigner_3j(F0,\u001b[38;5;241m1\u001b[39m,F1,\u001b[38;5;241m-\u001b[39mM0,\u001b[38;5;241m0\u001b[39m,M1)\u001b[38;5;241m*\u001b[39m\\\n\u001b[0;32m    384\u001b[0m         np\u001b[38;5;241m.\u001b[39msqrt((\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mF0\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mF1\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mF10\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mF11\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mN0\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mN1\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m*\u001b[39m\\\n\u001b[1;32m--> 385\u001b[0m         wigner_6j(F11,F1,iH,F0,F10,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mwigner_6j(N1,F11,G0,F10,N0,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mwigner_3j(N0,\u001b[38;5;241m1\u001b[39m,N1,\u001b[38;5;241m-\u001b[39mK0,\u001b[38;5;241m0\u001b[39m,K1)\n",
      "File \u001b[1;32mc:\\Users\\kyoto\\anaconda3\\Lib\\site-packages\\sympy\\physics\\wigner.py:577\u001b[0m, in \u001b[0;36mwigner_6j\u001b[1;34m(j_1, j_2, j_3, j_4, j_5, j_6, prec)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03mCalculate the Wigner 6j symbol `\\operatorname{Wigner6j}(j_1,j_2,j_3,j_4,j_5,j_6)`.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    572\u001b[0m \n\u001b[0;32m    573\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    574\u001b[0m j_1, j_2, j_3, j_4, j_5, j_6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(sympify, \\\n\u001b[0;32m    575\u001b[0m             [j_1, j_2, j_3, j_4, j_5, j_6])\n\u001b[0;32m    576\u001b[0m res \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mint\u001b[39m(j_1 \u001b[38;5;241m+\u001b[39m j_2 \u001b[38;5;241m+\u001b[39m j_4 \u001b[38;5;241m+\u001b[39m j_5) \u001b[38;5;241m*\u001b[39m \\\n\u001b[1;32m--> 577\u001b[0m     racah(j_1, j_2, j_5, j_4, j_3, j_6, prec)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\kyoto\\anaconda3\\Lib\\site-packages\\sympy\\physics\\wigner.py:453\u001b[0m, in \u001b[0;36mracah\u001b[1;34m(aa, bb, cc, dd, ee, ff, prec)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mracah\u001b[39m(aa, bb, cc, dd, ee, ff, prec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    400\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Calculate the Racah symbol `W(a,b,c,d;e,f)`.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    - Jens Rasch (2009-03-24): initial version\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    452\u001b[0m     prefac \u001b[38;5;241m=\u001b[39m _big_delta_coeff(aa, bb, ee, prec) \u001b[38;5;241m*\u001b[39m \\\n\u001b[1;32m--> 453\u001b[0m         _big_delta_coeff(cc, dd, ee, prec) \u001b[38;5;241m*\u001b[39m \\\n\u001b[0;32m    454\u001b[0m         _big_delta_coeff(aa, cc, ff, prec) \u001b[38;5;241m*\u001b[39m \\\n\u001b[0;32m    455\u001b[0m         _big_delta_coeff(bb, dd, ff, prec)\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefac \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m S\u001b[38;5;241m.\u001b[39mZero\n",
      "File \u001b[1;32mc:\\Users\\kyoto\\anaconda3\\Lib\\site-packages\\sympy\\physics\\wigner.py:385\u001b[0m, in \u001b[0;36m_big_delta_coeff\u001b[1;34m(aa, bb, cc, prec)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (bb \u001b[38;5;241m+\u001b[39m cc \u001b[38;5;241m-\u001b[39m aa) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m S\u001b[38;5;241m.\u001b[39mZero\n\u001b[1;32m--> 385\u001b[0m maxfact \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(aa \u001b[38;5;241m+\u001b[39m bb \u001b[38;5;241m-\u001b[39m cc, aa \u001b[38;5;241m+\u001b[39m cc \u001b[38;5;241m-\u001b[39m bb, bb \u001b[38;5;241m+\u001b[39m cc \u001b[38;5;241m-\u001b[39m aa, aa \u001b[38;5;241m+\u001b[39m bb \u001b[38;5;241m+\u001b[39m cc \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    386\u001b[0m _calc_factlist(maxfact)\n\u001b[0;32m    388\u001b[0m argsqrt \u001b[38;5;241m=\u001b[39m Integer(_Factlist[\u001b[38;5;28mint\u001b[39m(aa \u001b[38;5;241m+\u001b[39m bb \u001b[38;5;241m-\u001b[39m cc)] \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    389\u001b[0m                  _Factlist[\u001b[38;5;28mint\u001b[39m(aa \u001b[38;5;241m+\u001b[39m cc \u001b[38;5;241m-\u001b[39m bb)] \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    390\u001b[0m                  _Factlist[\u001b[38;5;28mint\u001b[39m(bb \u001b[38;5;241m+\u001b[39m cc \u001b[38;5;241m-\u001b[39m aa)]) \u001b[38;5;241m/\u001b[39m \\\n\u001b[0;32m    391\u001b[0m     Integer(_Factlist[\u001b[38;5;28mint\u001b[39m(aa \u001b[38;5;241m+\u001b[39m bb \u001b[38;5;241m+\u001b[39m cc \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)])\n",
      "File \u001b[1;32mc:\\Users\\kyoto\\anaconda3\\Lib\\site-packages\\sympy\\core\\decorators.py:76\u001b[0m, in \u001b[0;36m__sympifyit.<locals>.__sympifyit_wrapper\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(b, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_op_priority\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     75\u001b[0m         b \u001b[38;5;241m=\u001b[39m sympify(b, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(a, b)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SympifyError:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\kyoto\\anaconda3\\Lib\\site-packages\\sympy\\core\\numbers.py:1037\u001b[0m, in \u001b[0;36mFloat.__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Number) \u001b[38;5;129;01mand\u001b[39;00m global_parameters\u001b[38;5;241m.\u001b[39mevaluate:\n\u001b[0;32m   1036\u001b[0m     rhs, prec \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39m_as_mpf_op(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prec)\n\u001b[1;32m-> 1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Float\u001b[38;5;241m.\u001b[39m_new(mlib\u001b[38;5;241m.\u001b[39mmpf_sub(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mpf_, rhs, prec, rnd), prec)\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Number\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32mc:\\Users\\kyoto\\anaconda3\\Lib\\site-packages\\mpmath\\libmp\\libmpf.py:800\u001b[0m, in \u001b[0;36mmpf_sub\u001b[1;34m(s, t, prec, rnd)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmpf_sub\u001b[39m(s, t, prec\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, rnd\u001b[38;5;241m=\u001b[39mround_fast):\n\u001b[0;32m    798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the difference of two raw mpfs, s-t. This function is\u001b[39;00m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;124;03m    simply a wrapper of mpf_add that changes the sign of t.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mpf_add(s, t, prec, rnd, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kyoto\\anaconda3\\Lib\\site-packages\\mpmath\\libmp\\libmpf.py:778\u001b[0m, in \u001b[0;36mmpf_add\u001b[1;34m(s, t, prec, rnd, _sub)\u001b[0m\n\u001b[0;32m    776\u001b[0m             man \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mman\n\u001b[0;32m    777\u001b[0m             ssign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 778\u001b[0m     bc \u001b[38;5;241m=\u001b[39m bitcount(man)\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m normalize(ssign, man, texp, bc, prec \u001b[38;5;129;01mor\u001b[39;00m bc, rnd)\n\u001b[0;32m    780\u001b[0m \u001b[38;5;66;03m# Handle zeros and special numbers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kyoto\\anaconda3\\Lib\\site-packages\\mpmath\\libmp\\libintmath.py:93\u001b[0m, in \u001b[0;36mpython_bitcount\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpython_bitcount\u001b[39m(n):\n\u001b[0;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculate bit size of the nonnegative integer n.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     bc \u001b[38;5;241m=\u001b[39m bisect(powers, n)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bc \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m bc\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X010_173 = MoleculeLevels.initialize_state(\n",
    "    \"YbOH\",\n",
    "    \"173\",\n",
    "    \"X010\",\n",
    "    [1, 2],\n",
    "    M_values=\"all\",\n",
    "    I=[5 / 2, 1 / 2],\n",
    "    S=1 / 2,\n",
    "    round=8,\n",
    "    P_values=[1 / 2, 3 / 2],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X010_173 = MoleculeLevels.initialize_state(\n",
    "    \"YbOH\",\n",
    "    \"173\",\n",
    "    \"X010\",\n",
    "    [1, 2],\n",
    "    M_values=\"all\",\n",
    "    I=[5 / 2, 1 / 2],\n",
    "    S=1 / 2,\n",
    "    round=8,\n",
    "    P_values=[1 / 2, 3 / 2],\n",
    ")\n",
    "\n",
    "BASE_PARAMETERS = deepcopy(X010_173.parameters)\n",
    "FIT_PARAMETER_NAMES = [\n",
    "    \"Be\",\n",
    "    \"Gamma_SR\",\n",
    "    \"Gamma_Prime\",\n",
    "    \"bFYb\",\n",
    "    \"cYb\",\n",
    "    \"bFH\",\n",
    "    \"cH\",\n",
    "    \"e2Qq0\",\n",
    "    \"q_lD\",\n",
    "    \"p_lD\",\n",
    "    \"muE\",\n",
    "    \"g_S_eff\",\n",
    "]\n",
    "PARAM_PRIORS = {key: BASE_PARAMETERS[key] for key in FIT_PARAMETER_NAMES}\n",
    "\n",
    "\n",
    "def _default_bound(value, frac=0.2, floor=1e-6):\n",
    "    span = max(abs(value) * frac, floor)\n",
    "    return value - span, value + span\n",
    "\n",
    "\n",
    "FIT_PARAMETER_BOUNDS = {key: _default_bound(PARAM_PRIORS[key]) for key in FIT_PARAMETER_NAMES}\n",
    "\n",
    "print(f\"Loaded {X010_173.iso_state} with {len(FIT_PARAMETER_NAMES)} fit parameters tracked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d2fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Synthetic \"experimental\" peak generator for this repo (windowed & sparse) ---\n",
    "# Run this AFTER you have X010_173 initialized.\n",
    "# Goal: for EACH spectrum, produce 5–10 observed peaks between 300 and 400 MHz.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------- helpers -------------------\n",
    "def _to_numpy_1d(x):\n",
    "    \"\"\"Convert torch/numpy/list to a 1D float numpy array.\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return x.detach().cpu().numpy().astype(float).ravel()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(x, dtype=float).ravel()\n",
    "\n",
    "def get_evals(state, Ez, Bz, method=\"torch\"):\n",
    "    \"\"\"\n",
    "    Robustly get eigenvalues from MoleculeLevels regardless of return style.\n",
    "    Uses set_attr=True then reads state.evals if needed.\n",
    "    \"\"\"\n",
    "    out = state.eigensystem(Ez, Bz, order=True, method=method, set_attr=True)\n",
    "    if out is not None:\n",
    "        evals = out[0]\n",
    "    else:\n",
    "        evals = getattr(state, \"evals\", None)\n",
    "        if evals is None:\n",
    "            raise RuntimeError(\"Couldn't find eigenvalues. state.eigensystem() didn't return and state.evals not set.\")\n",
    "    evals = _to_numpy_1d(evals)\n",
    "    if evals.size < 2:\n",
    "        raise RuntimeError(f\"Too few eigenvalues ({evals.size}). Basis/truncation may be too small.\")\n",
    "    return evals\n",
    "\n",
    "def predict_lines_from_levels(evals,\n",
    "                             n_levels=120,\n",
    "                             fmin=300.0,\n",
    "                             fmax=400.0,\n",
    "                             max_lines=50000,\n",
    "                             seed=0):\n",
    "    \"\"\"\n",
    "    Candidate transition frequencies from pairwise differences of the lowest n_levels energies.\n",
    "    NOT enforcing selection rules (pipeline testing only).\n",
    "\n",
    "    Returns only lines within [fmin, fmax] MHz (no auto-expansion).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    e = np.sort(_to_numpy_1d(evals))\n",
    "    n = min(int(n_levels), e.size)\n",
    "    if n < 2:\n",
    "        return np.array([], dtype=float)\n",
    "    e = e[:n]\n",
    "\n",
    "    # Positive pairwise differences: e[j] - e[i] for j>i\n",
    "    diffs = (e[None, :] - e[:, None])[np.triu_indices(n, k=1)]\n",
    "    diffs = diffs[(diffs >= fmin) & (diffs <= fmax)]\n",
    "\n",
    "    if diffs.size == 0:\n",
    "        return np.array([], dtype=float)\n",
    "\n",
    "    diffs = np.unique(np.round(np.sort(diffs), 6))\n",
    "\n",
    "    # If too many, subsample to max_lines for speed\n",
    "    if diffs.size > max_lines:\n",
    "        idx = rng.choice(diffs.size, size=max_lines, replace=False)\n",
    "        diffs = np.sort(diffs[idx])\n",
    "\n",
    "    return diffs.astype(float)\n",
    "\n",
    "def make_fake_experiment_in_window(lines,\n",
    "                                   nu_min=300.0,\n",
    "                                   nu_max=400.0,\n",
    "                                   n_keep_min=5,\n",
    "                                   n_keep_max=10,\n",
    "                                   sigma_MHz=0.25,\n",
    "                                   spurious=1,\n",
    "                                   seed=0):\n",
    "    \"\"\"\n",
    "    Create sparse, unassigned observed peaks in [nu_min, nu_max] MHz.\n",
    "\n",
    "    - Picks a random number of peaks between n_keep_min and n_keep_max from the candidate lines.\n",
    "    - If not enough candidate lines exist, fills remaining peaks uniformly in-window (spurious).\n",
    "    - Adds Gaussian jitter sigma_MHz to mimic centroid uncertainty.\n",
    "    - Adds extra spurious peaks (optional) uniformly in-window.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    lines = _to_numpy_1d(lines)\n",
    "\n",
    "    # Ensure windowing (should already be windowed, but keep safe)\n",
    "    inwin = lines[(lines >= nu_min) & (lines <= nu_max)]\n",
    "    inwin = np.unique(np.round(np.sort(inwin), 6))\n",
    "\n",
    "    n_target = int(rng.integers(int(n_keep_min), int(n_keep_max) + 1))\n",
    "\n",
    "    if inwin.size >= n_target:\n",
    "        idx = rng.choice(inwin.size, size=n_target, replace=False)\n",
    "        obs = np.sort(inwin[idx])\n",
    "    else:\n",
    "        obs = inwin.copy()\n",
    "        n_missing = n_target - obs.size\n",
    "        fill = rng.uniform(nu_min, nu_max, size=n_missing)\n",
    "        obs = np.sort(np.concatenate([obs, fill]))\n",
    "\n",
    "    # Add measurement jitter and clip to window\n",
    "    obs = obs + rng.normal(0.0, sigma_MHz, size=obs.size)\n",
    "    obs = np.clip(obs, nu_min, nu_max)\n",
    "\n",
    "    # Add extra spurious peaks (optional)\n",
    "    if spurious and int(spurious) > 0:\n",
    "        spur = rng.uniform(nu_min, nu_max, size=int(spurious))\n",
    "        obs = np.sort(np.concatenate([obs, spur]))\n",
    "\n",
    "    return np.sort(obs)\n",
    "\n",
    "def generate_synthetic_peaks_csv_windowed(state,\n",
    "                                         conditions,\n",
    "                                         output_csv=\"synthetic_peaks.csv\",\n",
    "                                         method=\"torch\",\n",
    "                                         # window constraints\n",
    "                                         window_min=300.0,\n",
    "                                         window_max=400.0,\n",
    "                                         # eigenlevel sampling\n",
    "                                         n_levels_start=120,\n",
    "                                         n_levels_max=600,\n",
    "                                         # observed sparsity\n",
    "                                         n_keep_min=5,\n",
    "                                         n_keep_max=10,\n",
    "                                         sigma_MHz=0.25,\n",
    "                                         spurious=1,\n",
    "                                         seed_base=0,\n",
    "                                         verbose=True):\n",
    "    \"\"\"\n",
    "    Generate a multi-spectrum unassigned peak CSV with 5–10 peaks per spectrum\n",
    "    in a specified frequency window [window_min, window_max] MHz.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for idx, (spec_id, Ez, Bz) in enumerate(conditions):\n",
    "        evals = get_evals(state, Ez, Bz, method=method)\n",
    "\n",
    "        # Try increasing n_levels until we get some lines in the window\n",
    "        lines = np.array([], dtype=float)\n",
    "        n_levels = int(n_levels_start)\n",
    "\n",
    "        while lines.size == 0 and n_levels <= int(n_levels_max):\n",
    "            lines = predict_lines_from_levels(\n",
    "                evals,\n",
    "                n_levels=n_levels,\n",
    "                fmin=window_min,\n",
    "                fmax=window_max,\n",
    "                max_lines=50000,\n",
    "                seed=seed_base + 100 + idx,\n",
    "            )\n",
    "            n_levels = int(np.ceil(n_levels * 1.5))\n",
    "\n",
    "        # If still none, widen the window slightly (still centered around requested region)\n",
    "        if lines.size == 0:\n",
    "            lines = predict_lines_from_levels(\n",
    "                evals,\n",
    "                n_levels=min(int(n_levels_max), evals.size),\n",
    "                fmin=window_min - 50.0,\n",
    "                fmax=window_max + 50.0,\n",
    "                max_lines=50000,\n",
    "                seed=seed_base + 200 + idx,\n",
    "            )\n",
    "\n",
    "        if lines.size == 0:\n",
    "            raise RuntimeError(\n",
    "                f\"[{spec_id}] No candidate lines found near {window_min}-{window_max} MHz \"\n",
    "                f\"(even after widening). Try increasing n_levels_max or widening the window.\"\n",
    "            )\n",
    "\n",
    "        peaks = make_fake_experiment_in_window(\n",
    "            lines,\n",
    "            nu_min=window_min,\n",
    "            nu_max=window_max,\n",
    "            n_keep_min=n_keep_min,\n",
    "            n_keep_max=n_keep_max,\n",
    "            sigma_MHz=sigma_MHz,\n",
    "            spurious=spurious,\n",
    "            seed=seed_base + 300 + idx,\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{spec_id}: Ez={Ez} V/cm, Bz={Bz} G | evals={evals.size} | \"\n",
    "                  f\"cand_lines_in_window={lines.size} | peaks_written={peaks.size} | \"\n",
    "                  f\"window=[{window_min},{window_max}] MHz\")\n",
    "\n",
    "        for nu in peaks:\n",
    "            rows.append({\n",
    "                \"spectrum_id\": spec_id,\n",
    "                \"Ez_V_per_cm\": float(Ez),\n",
    "                \"Bz_G\": float(Bz),\n",
    "                \"nu_obs_MHz\": float(nu),\n",
    "                \"sigma_MHz\": float(sigma_MHz),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values([\"spectrum_id\", \"nu_obs_MHz\"]).reset_index(drop=True)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return df\n",
    "\n",
    "# ------------------- Example usage -------------------\n",
    "spectra_conditions = [\n",
    "    (\"spec0\", 0.0, 0.0),\n",
    "    (\"spec1\", 20.0, 5.0),\n",
    "    (\"spec2\", 40.0, 5.0),\n",
    "    (\"spec3\", 60.0, 10.0),\n",
    "]\n",
    "\n",
    "df = generate_synthetic_peaks_csv_windowed(\n",
    "    X010_173,\n",
    "    spectra_conditions,\n",
    "    output_csv=\"synthetic_peaks.csv\",\n",
    "    method=\"torch\",\n",
    "    window_min=300.0,\n",
    "    window_max=400.0,\n",
    "    n_levels_start=120,\n",
    "    n_levels_max=600,\n",
    "    n_keep_min=5,\n",
    "    n_keep_max=10,\n",
    "    sigma_MHz=0.25,\n",
    "    spurious=1,          # set 0 for none\n",
    "    seed_base=123,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "df.head(20)\n",
    "# File \"synthetic_peaks.csv\" now has ~5–10 peaks per spectrum in 300–400 MHz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f776b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_state_parameters(state, updates=None):\n",
    "    if updates is None:\n",
    "        updates = {}\n",
    "    new_params = dict(state.parameters)\n",
    "    new_params.update(updates)\n",
    "\n",
    "    state.parameters = new_params\n",
    "    state.library.parameters[state.iso_state] = new_params\n",
    "\n",
    "    b = state.library.H_builders[state.iso_state]\n",
    "    state.H_function, state.H_symbolic = b(\n",
    "        state.q_numbers,\n",
    "        params=new_params,\n",
    "        M_values=state.M_values,\n",
    "        precision=state.round,\n",
    "    )\n",
    "\n",
    "    state.eigensystem(0, 1e-8, order=True, method=\"torch\", set_attr=True)\n",
    "    state.generate_parities(state.evecs0)\n",
    "\n",
    "\n",
    "\n",
    "def parameter_vector_to_dict(vector):\n",
    "    return {name: value for name, value in zip(FIT_PARAMETER_NAMES, vector)}\n",
    "\n",
    "\n",
    "def current_parameter_dict(state=None):\n",
    "    source = state.parameters if state is not None else BASE_PARAMETERS\n",
    "    return {name: source[name] for name in FIT_PARAMETER_NAMES}\n",
    "\n",
    "\n",
    "def parameters_to_vector(params):\n",
    "    return np.array([params[name] for name in FIT_PARAMETER_NAMES], dtype=float)\n",
    "\n",
    "set_state_parameters(X010_173)\n",
    "baseline_parameter_vector = parameters_to_vector(current_parameter_dict(X010_173))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c6ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental data setup and frequency transform configuration\n",
    "EXPERIMENTAL_DATA_PATH = Path(\"/absolute/path/to/your/experimental_assignments.csv\")  # TODO: update\n",
    "REQUIRED_COLUMNS = [\"state index 0\", \"state index 1\", \"freq_obs\"]\n",
    "\n",
    "if EXPERIMENTAL_DATA_PATH.exists():\n",
    "    observed_df = pd.read_csv(EXPERIMENTAL_DATA_PATH)\n",
    "    observed_df.columns = [col.strip() for col in observed_df.columns]\n",
    "    missing = [col for col in REQUIRED_COLUMNS if col not in observed_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns in experimental data: {missing}\")\n",
    "else:\n",
    "    observed_df = pd.DataFrame(columns=REQUIRED_COLUMNS + [\"uncertainty\"])\n",
    "    print(\"Experimental data file not found. Update EXPERIMENTAL_DATA_PATH and re-run this cell.\")\n",
    "\n",
    "if \"uncertainty\" not in observed_df.columns:\n",
    "    observed_df[\"uncertainty\"] = 1.0  # MHz weights (set to 1 if unknown)\n",
    "\n",
    "observed_df = observed_df.copy()\n",
    "for col in [\"freq_obs\", \"uncertainty\"]:\n",
    "    if col in observed_df.columns:\n",
    "        observed_df[col] = pd.to_numeric(observed_df[col], errors=\"coerce\")\n",
    "\n",
    "observed_df = observed_df.dropna(subset=[\"freq_obs\"]).reset_index(drop=True)\n",
    "\n",
    "# Frequency transform controls (matching the previous plotting convention)\n",
    "FREQ_OFFSET = (106.089 - 97.58) * 2  # MHz\n",
    "FREQ_SCALE = 0.5  # Divide by two for two-photon frequency conversion\n",
    "FREQ_SHIFT = -90.0  # Additional shift applied after scaling\n",
    "\n",
    "\n",
    "def model_frequency_transform(raw_freq):\n",
    "    \"\"\"Map raw transition frequency from the model to the experimental frequency axis.\"\"\"\n",
    "    return (raw_freq - FREQ_OFFSET) * FREQ_SCALE + FREQ_SHIFT\n",
    "\n",
    "\n",
    "TRANSITION_INDEX_SET = None  # Replace with a list of indices to restrict transitions if desired\n",
    "LASER_POLARIZATION = \"orth\"\n",
    "PARITY_SIGN = 1\n",
    "INTENSITY_THRESHOLD = None  # Set to a float to discard transitions with weaker intensity\n",
    "EZ_FIELD = 40  # Update if the experimental conditions change\n",
    "B_FIELD = 1e-8\n",
    "RUN_OPTIMIZATION = False  # Set to True to launch least-squares fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b599a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_transitions(\n",
    "    state,\n",
    "    Ez=EZ_FIELD,\n",
    "    B=B_FIELD,\n",
    "    indices=None,\n",
    "    parity_sign=PARITY_SIGN,\n",
    "    polarization=LASER_POLARIZATION,\n",
    "):\n",
    "    index_list = indices if indices is not None else TRANSITION_INDEX_SET\n",
    "    if index_list is None:\n",
    "        index_list = list(range(84))\n",
    "\n",
    "    result = state.calculate_two_photon_spectrum(\n",
    "        Ez,\n",
    "        B,\n",
    "        index_list,\n",
    "        parity_sign=parity_sign,\n",
    "        laser_polarization=polarization,\n",
    "    )\n",
    "    transitions = pd.DataFrame(result[1])\n",
    "\n",
    "    if not transitions.empty:\n",
    "        transitions[\"freq_model\"] = transitions[\"freq\"].apply(model_frequency_transform)\n",
    "        if INTENSITY_THRESHOLD is not None:\n",
    "            intensity_key = next(\n",
    "                (key for key in [\"intensity\", \"Intensity\", \"strength\"] if key in transitions.columns),\n",
    "                None,\n",
    "            )\n",
    "            if intensity_key is not None:\n",
    "                transitions = transitions[transitions[intensity_key] >= INTENSITY_THRESHOLD].reset_index(drop=True)\n",
    "\n",
    "    return transitions, result\n",
    "\n",
    "\n",
    "def merge_predictions_with_experiment(predicted_df, experimental_df):\n",
    "    if experimental_df.empty:\n",
    "        predicted_df = predicted_df.copy()\n",
    "        predicted_df[\"freq_obs\"] = np.nan\n",
    "        predicted_df[\"residual\"] = np.nan\n",
    "        predicted_df[\"weight\"] = 1.0\n",
    "        return predicted_df, pd.DataFrame(), predicted_df\n",
    "\n",
    "    merge_cols = [\"state index 0\", \"state index 1\"]\n",
    "    for col in [\"M0\", \"M1\"]:\n",
    "        if col in experimental_df.columns and col in predicted_df.columns:\n",
    "            merge_cols.append(col)\n",
    "\n",
    "    merged = experimental_df.merge(predicted_df, how=\"left\", on=merge_cols, suffixes=(\"_obs\", \"_model\"))\n",
    "    missing = merged[merged[\"freq_model\"].isna()].copy()\n",
    "\n",
    "    merged[\"freq_model\"] = merged[\"freq_model\"].astype(float)\n",
    "    merged[\"residual\"] = merged[\"freq_model\"] - merged[\"freq_obs\"]\n",
    "    if \"uncertainty\" in merged.columns:\n",
    "        weights = merged[\"uncertainty\"].replace(0, np.nan).fillna(1.0)\n",
    "    else:\n",
    "        weights = pd.Series(1.0, index=merged.index)\n",
    "    merged[\"weight\"] = weights\n",
    "    merged[\"weighted_residual\"] = merged[\"residual\"] / merged[\"weight\"]\n",
    "\n",
    "    matched = merged[merged[\"freq_model\"].notna()].copy()\n",
    "    return matched, missing, predicted_df\n",
    "\n",
    "\n",
    "def summarize_fit(matched_df):\n",
    "    if matched_df.empty:\n",
    "        return {\"rms\": np.nan, \"weighted_rms\": np.nan, \"n_points\": 0}\n",
    "\n",
    "    valid = np.isfinite(matched_df[\"residual\"]) & np.isfinite(matched_df[\"weight\"])\n",
    "    if not valid.any():\n",
    "        return {\"rms\": np.nan, \"weighted_rms\": np.nan, \"n_points\": 0}\n",
    "\n",
    "    residuals = matched_df.loc[valid, \"residual\"].to_numpy()\n",
    "    weights = matched_df.loc[valid, \"weight\"].to_numpy()\n",
    "    rms = np.sqrt(np.mean(residuals**2))\n",
    "    weighted_rms = np.sqrt(np.mean((residuals / weights) ** 2))\n",
    "    return {\"rms\": rms, \"weighted_rms\": weighted_rms, \"n_points\": int(valid.sum())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predicted_df, baseline_raw = compute_model_transitions(X010_173)\n",
    "baseline_matched_df, baseline_missing_df, baseline_predicted_df = merge_predictions_with_experiment(\n",
    "    baseline_predicted_df, observed_df\n",
    ")\n",
    "baseline_summary = summarize_fit(baseline_matched_df)\n",
    "\n",
    "print(\"Baseline comparison summary:\", baseline_summary)\n",
    "if not baseline_missing_df.empty:\n",
    "    print(f\"Unmatched experimental assignments: {len(baseline_missing_df)}\")\n",
    "\n",
    "\n",
    "def weighted_residual_vector(param_vector):\n",
    "    updates = parameter_vector_to_dict(param_vector)\n",
    "    set_state_parameters(X010_173, updates)\n",
    "    predicted_df, _ = compute_model_transitions(X010_173)\n",
    "    matched_df, missing_df, _ = merge_predictions_with_experiment(predicted_df, observed_df)\n",
    "    if matched_df.empty:\n",
    "        raise ValueError(\n",
    "            \"No model transitions matched the experimental assignments. \"\n",
    "            \"Adjust TRANSITION_INDEX_SET or check the assignments.\"\n",
    "        )\n",
    "    return matched_df[\"weighted_residual\"].to_numpy()\n",
    "\n",
    "\n",
    "optimization_result = None\n",
    "fit_parameters = parameter_vector_to_dict(baseline_parameter_vector)\n",
    "fit_predicted_df = baseline_predicted_df\n",
    "fit_matched_df = baseline_matched_df\n",
    "fit_missing_df = baseline_missing_df\n",
    "fit_summary = baseline_summary\n",
    "\n",
    "if RUN_OPTIMIZATION and not observed_df.empty:\n",
    "    lower_bounds = np.array([FIT_PARAMETER_BOUNDS[name][0] for name in FIT_PARAMETER_NAMES])\n",
    "    upper_bounds = np.array([FIT_PARAMETER_BOUNDS[name][1] for name in FIT_PARAMETER_NAMES])\n",
    "\n",
    "    optimization_result = least_squares(\n",
    "        weighted_residual_vector,\n",
    "        x0=baseline_parameter_vector,\n",
    "        bounds=(lower_bounds, upper_bounds),\n",
    "        verbose=2,\n",
    "    )\n",
    "    fit_parameters = parameter_vector_to_dict(optimization_result.x)\n",
    "    set_state_parameters(X010_173, fit_parameters)\n",
    "    fit_predicted_df, _ = compute_model_transitions(X010_173)\n",
    "    fit_matched_df, fit_missing_df, fit_predicted_df = merge_predictions_with_experiment(\n",
    "        fit_predicted_df, observed_df\n",
    "    )\n",
    "    fit_summary = summarize_fit(fit_matched_df)\n",
    "    print(\"Fit summary:\", fit_summary)\n",
    "\n",
    "fit_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e705e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not fit_matched_df.empty:\n",
    "    display(fit_matched_df[[\"state index 0\", \"state index 1\", \"freq_obs\", \"freq_model\", \"residual\"]].head())\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.errorbar(\n",
    "        fit_matched_df[\"freq_obs\"],\n",
    "        fit_matched_df[\"freq_model\"],\n",
    "        yerr=fit_matched_df.get(\"uncertainty\", pd.Series(0, index=fit_matched_df.index)),\n",
    "        fmt=\"o\",\n",
    "        ms=6,\n",
    "        alpha=0.7,\n",
    "        label=\"Matched transitions\",\n",
    "    )\n",
    "    lims = [\n",
    "        min(fit_matched_df[\"freq_obs\"].min(), fit_matched_df[\"freq_model\"].min()) - 0.1,\n",
    "        max(fit_matched_df[\"freq_obs\"].max(), fit_matched_df[\"freq_model\"].max()) + 0.1,\n",
    "    ]\n",
    "    ax.plot(lims, lims, \"k--\", label=\"Perfect agreement\")\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "    ax.set_xlabel(\"Observed frequency (MHz)\")\n",
    "    ax.set_ylabel(\"Model frequency (MHz)\")\n",
    "    ax.set_title(\"Observed vs. model transition frequencies\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No matched transitions to visualize yet. Load experimental data and rerun the fit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b685cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unassigned frequency data configuration\n",
    "UNASSIGNED_DATA_PATH = Path(\"/absolute/path/to/your/unassigned_frequencies.csv\")  # TODO: update\n",
    "UNASSIGNED_FREQ_COLUMN = \"freq_obs\"  # Column name in the CSV file\n",
    "UNASSIGNED_WEIGHT_COLUMN = None  # Set to column name if relative intensities are available\n",
    "UNASSIGNED_SIGMA = 0.03  # MHz Gaussian width used to broaden stick spectra\n",
    "\n",
    "if UNASSIGNED_DATA_PATH.exists():\n",
    "    unassigned_df = pd.read_csv(UNASSIGNED_DATA_PATH)\n",
    "    if UNASSIGNED_FREQ_COLUMN not in unassigned_df.columns:\n",
    "        raise ValueError(\n",
    "            f\"Column '{UNASSIGNED_FREQ_COLUMN}' not found in unassigned data: {unassigned_df.columns.tolist()}\"\n",
    "        )\n",
    "    observed_unassigned_freqs = pd.to_numeric(\n",
    "        unassigned_df[UNASSIGNED_FREQ_COLUMN], errors=\"coerce\"\n",
    "    ).dropna().to_numpy()\n",
    "    if UNASSIGNED_WEIGHT_COLUMN and UNASSIGNED_WEIGHT_COLUMN in unassigned_df.columns:\n",
    "        observed_unassigned_weights = pd.to_numeric(\n",
    "            unassigned_df[UNASSIGNED_WEIGHT_COLUMN], errors=\"coerce\"\n",
    "        ).fillna(1.0).to_numpy()\n",
    "    else:\n",
    "        observed_unassigned_weights = np.ones_like(observed_unassigned_freqs)\n",
    "    print(f\"Loaded {len(observed_unassigned_freqs)} unassigned transition frequencies.\")\n",
    "else:\n",
    "    observed_unassigned_freqs = np.array([])\n",
    "    observed_unassigned_weights = np.array([])\n",
    "    print(\"Unassigned frequency data not found. Update UNASSIGNED_DATA_PATH and re-run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd6b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_broadened_spectrum(frequencies, weights=None, freq_axis=None, sigma=UNASSIGNED_SIGMA):\n",
    "    if freq_axis is None:\n",
    "        if frequencies.size == 0:\n",
    "            return np.linspace(0, 1, 1000), np.zeros(1000)\n",
    "        f_min, f_max = frequencies.min() - 3 * sigma, frequencies.max() + 3 * sigma\n",
    "        freq_axis = np.linspace(f_min, f_max, 2000)\n",
    "    if weights is None:\n",
    "        weights = np.ones_like(frequencies)\n",
    "    if frequencies.size == 0:\n",
    "        return freq_axis, np.zeros_like(freq_axis)\n",
    "    diff = freq_axis[:, None] - frequencies[None, :]\n",
    "    spectrum = np.exp(-(diff**2) / (2 * sigma**2)) @ weights\n",
    "    return freq_axis, spectrum\n",
    "\n",
    "\n",
    "def spectral_residual(predicted_freqs, observed_freqs, predicted_weights=None, observed_weights=None, sigma=UNASSIGNED_SIGMA):\n",
    "    if observed_freqs.size == 0 or predicted_freqs.size == 0:\n",
    "        return np.inf\n",
    "    freq_axis, observed_spec = gaussian_broadened_spectrum(observed_freqs, observed_weights, sigma=sigma)\n",
    "    _, predicted_spec = gaussian_broadened_spectrum(predicted_freqs, predicted_weights, freq_axis=freq_axis, sigma=sigma)\n",
    "    observed_spec /= observed_spec.max() if observed_spec.max() else 1\n",
    "    predicted_spec /= predicted_spec.max() if predicted_spec.max() else 1\n",
    "    return np.sqrt(np.mean((predicted_spec - observed_spec) ** 2))\n",
    "\n",
    "\n",
    "def transition_frequency_set(state, Ez=EZ_FIELD, B=B_FIELD, indices=None, **kwargs):\n",
    "    transitions, raw = compute_model_transitions(state, Ez=Ez, B=B, indices=indices, **kwargs)\n",
    "    if transitions.empty:\n",
    "        return np.array([]), np.array([]), raw\n",
    "    weights = None\n",
    "    for candidate in [\"intensity\", \"Intensity\", \"strength\", \"Strength\"]:\n",
    "        if candidate in transitions.columns:\n",
    "            weights = transitions[candidate].to_numpy()\n",
    "            break\n",
    "    return transitions[\"freq_model\"].to_numpy(), weights, raw\n",
    "\n",
    "\n",
    "def unassigned_spectrum_loss(state, Ez=EZ_FIELD, B=B_FIELD, sigma=UNASSIGNED_SIGMA, observed_freqs=None, observed_weights=None, **kwargs):\n",
    "    if observed_freqs is None:\n",
    "        observed_freqs = observed_unassigned_freqs\n",
    "    if observed_weights is None:\n",
    "        observed_weights = observed_unassigned_weights\n",
    "    predicted_freqs, predicted_weights, _ = transition_frequency_set(state, Ez=Ez, B=B, **kwargs)\n",
    "    return spectral_residual(predicted_freqs, observed_freqs, predicted_weights, observed_weights, sigma=sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0748d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter space exploration for unassigned spectra\n",
    "UNASSIGNED_PARAMETER_NAMES = FIT_PARAMETER_NAMES  # Override to restrict search\n",
    "UNASSIGNED_PARAMETER_BOUNDS = {name: FIT_PARAMETER_BOUNDS[name] for name in UNASSIGNED_PARAMETER_NAMES}\n",
    "UNASSIGNED_RANDOM_SAMPLES = 100\n",
    "UNASSIGNED_TOP_K = 10\n",
    "USE_GAUSSIAN_PROCESS = True  # Toggle Bayesian surrogate refinement\n",
    "\n",
    "\n",
    "def sample_parameter_vector(bounds_dict):\n",
    "    return np.array([\n",
    "        np.random.uniform(low=low, high=high) for (low, high) in bounds_dict.values()\n",
    "    ])\n",
    "\n",
    "\n",
    "def evaluate_parameter_vector(param_vector, state=None, observed_freqs=None, observed_weights=None):\n",
    "    state = state or X010_173\n",
    "    updates = parameter_vector_to_dict(param_vector)\n",
    "    set_state_parameters(state, updates)\n",
    "    loss = unassigned_spectrum_loss(\n",
    "        state,\n",
    "        observed_freqs=observed_freqs,\n",
    "        observed_weights=observed_weights,\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def random_explore_unassigned(state, n_samples=UNASSIGNED_RANDOM_SAMPLES, bounds=None, observed_freqs=None, observed_weights=None):\n",
    "    bounds = bounds or UNASSIGNED_PARAMETER_BOUNDS\n",
    "    names = list(bounds.keys())\n",
    "    samples = []\n",
    "    for _ in range(n_samples):\n",
    "        vec = sample_parameter_vector(bounds)\n",
    "        loss = evaluate_parameter_vector(vec, state=state, observed_freqs=observed_freqs, observed_weights=observed_weights)\n",
    "        samples.append({\"params\": parameter_vector_to_dict(vec), \"vector\": vec, \"loss\": loss})\n",
    "    samples.sort(key=lambda x: x[\"loss\"])\n",
    "    return names, samples\n",
    "\n",
    "\n",
    "random_search_results = None\n",
    "if observed_unassigned_freqs.size:\n",
    "    names, random_search_results = random_explore_unassigned(\n",
    "        X010_173,\n",
    "        n_samples=UNASSIGNED_RANDOM_SAMPLES,\n",
    "        bounds=UNASSIGNED_PARAMETER_BOUNDS,\n",
    "        observed_freqs=observed_unassigned_freqs,\n",
    "        observed_weights=observed_unassigned_weights,\n",
    "    )\n",
    "    best_candidates = random_search_results[:UNASSIGNED_TOP_K]\n",
    "    print(\"Top random-search candidates (loss is spectrum RMSE):\")\n",
    "    for idx, candidate in enumerate(best_candidates, 1):\n",
    "        print(f\"#{idx}: loss={candidate['loss']:.5f}\")\n",
    "else:\n",
    "    print(\"No unassigned data loaded; skipping random exploration.\")\n",
    "\n",
    "\n",
    "surrogate_model = None\n",
    "surrogate_history = []\n",
    "\n",
    "if USE_GAUSSIAN_PROCESS and observed_unassigned_freqs.size and random_search_results:\n",
    "    try:\n",
    "        from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "        from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "        kernel = Matern(length_scale=np.ones(len(names)), nu=2.5) + WhiteKernel(noise_level=1e-6)\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, normalize_y=True, n_restarts_optimizer=3)\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        X_samples = np.array([entry[\"vector\"] for entry in random_search_results])\n",
    "        y_samples = np.array([entry[\"loss\"] for entry in random_search_results])\n",
    "        scaler.fit(X_samples)\n",
    "        gp.fit(scaler.transform(X_samples), y_samples)\n",
    "\n",
    "        surrogate_model = {\"gp\": gp, \"scaler\": scaler, \"names\": names}\n",
    "\n",
    "        def propose_next(gp_model, n_candidates=512):\n",
    "            raw = np.array([sample_parameter_vector(UNASSIGNED_PARAMETER_BOUNDS) for _ in range(n_candidates)])\n",
    "            mu, sigma = gp_model[\"gp\"].predict(gp_model[\"scaler\"].transform(raw), return_std=True)\n",
    "            acquisition = mu - 1.96 * sigma  # Lower Confidence Bound\n",
    "            best_idx = np.argmin(acquisition)\n",
    "            return raw[best_idx], acquisition[best_idx]\n",
    "\n",
    "        for iteration in range(10):\n",
    "            candidate_vec, acquisition_value = propose_next(surrogate_model)\n",
    "            candidate_loss = evaluate_parameter_vector(\n",
    "                candidate_vec,\n",
    "                state=X010_173,\n",
    "                observed_freqs=observed_unassigned_freqs,\n",
    "                observed_weights=observed_unassigned_weights,\n",
    "            )\n",
    "            surrogate_history.append({\n",
    "                \"vector\": candidate_vec,\n",
    "                \"params\": parameter_vector_to_dict(candidate_vec),\n",
    "                \"loss\": candidate_loss,\n",
    "                \"acquisition\": acquisition_value,\n",
    "            })\n",
    "            # Update GP with new observation\n",
    "            X_samples = np.vstack([X_samples, candidate_vec])\n",
    "            y_samples = np.append(y_samples, candidate_loss)\n",
    "            scaler.fit(X_samples)\n",
    "            gp.fit(scaler.transform(X_samples), y_samples)\n",
    "\n",
    "        print(f\"Gaussian-process refinement completed with {len(surrogate_history)} additional evaluations.\")\n",
    "    except ImportError:\n",
    "        print(\"scikit-learn not available; skipping Gaussian-process refinement.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c469b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_candidate_table(random_results=None, surrogate_results=None, top_k=UNASSIGNED_TOP_K):\n",
    "    records = []\n",
    "    for source, entries in [(\"random\", random_results or []), (\"gp\", surrogate_results or [])]:\n",
    "        for entry in (entries if isinstance(entries, list) else []):\n",
    "            row = {\"source\": source, \"loss\": entry[\"loss\"], **entry.get(\"params\", {})}\n",
    "            records.append(row)\n",
    "    if not records:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(records)\n",
    "    df = df.sort_values(\"loss\").head(top_k).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "candidate_table = collect_candidate_table(\n",
    "    random_results=random_search_results,\n",
    "    surrogate_results=surrogate_history,\n",
    ")\n",
    "\n",
    "if not candidate_table.empty:\n",
    "    display(candidate_table)\n",
    "else:\n",
    "    print(\"No parameter candidates generated yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrum_comparison(candidate_params, observed_freqs=None, observed_weights=None, sigma=UNASSIGNED_SIGMA, title_suffix=\"\"):\n",
    "    observed_freqs = observed_freqs if observed_freqs is not None else observed_unassigned_freqs\n",
    "    observed_weights = observed_weights if observed_weights is not None else observed_unassigned_weights\n",
    "    if observed_freqs.size == 0:\n",
    "        print(\"No unassigned frequencies to visualize.\")\n",
    "        return\n",
    "    set_state_parameters(X010_173, candidate_params)\n",
    "    predicted_freqs, predicted_weights, _ = transition_frequency_set(X010_173)\n",
    "    freq_axis, observed_spec = gaussian_broadened_spectrum(observed_freqs, observed_weights, sigma=sigma)\n",
    "    _, predicted_spec = gaussian_broadened_spectrum(predicted_freqs, predicted_weights, freq_axis=freq_axis, sigma=sigma)\n",
    "    observed_spec /= observed_spec.max() if observed_spec.max() else 1\n",
    "    predicted_spec /= predicted_spec.max() if predicted_spec.max() else 1\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(freq_axis, observed_spec, label=\"Observed\", lw=2)\n",
    "    plt.plot(freq_axis, predicted_spec, label=\"Predicted\", lw=2)\n",
    "    plt.xlabel(\"Frequency (MHz)\")\n",
    "    plt.ylabel(\"Normalized intensity\")\n",
    "    plt.title(f\"Spectrum overlay {title_suffix}\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if candidate_table.shape[0]:\n",
    "    best_candidate_params = candidate_table.iloc[0].drop(labels=[\"source\", \"loss\"]).to_dict()\n",
    "    print(\"Plotting spectrum overlay for top candidate...\")\n",
    "    plot_spectrum_comparison(best_candidate_params, title_suffix=f\"loss={candidate_table.iloc[0]['loss']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
